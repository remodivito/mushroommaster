{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "tWjBIw-rB7mT",
    "outputId": "ffea2944-996d-4f0a-8171-37cb74f7f187"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from google.cloud import storage\n",
    "from io import BytesIO\n",
    "from google.colab import auth\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "import glob\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "auth.authenticate_user()\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!pip install --upgrade tensorflow\n",
    "\n",
    "project_id = \"mushroom-master-136c0\"\n",
    "bucket_name = \"mushroom-master-central\"\n",
    "source_directory = \"cleaned_dataset/\"\n",
    "\n",
    "\n",
    "client = storage.Client(project=project_id)\n",
    "bucket = client.bucket(bucket_name)\n",
    "\n",
    "!mkdir -p /content/local_data/cleaned_dataset\n",
    "!gsutil -m rsync -r gs://{bucket_name}/{source_directory} /content/local_data/cleaned_dataset\n",
    "\n",
    "local_data_dir = \"/content/local_data/cleaned_dataset\"\n",
    "\n",
    "model_path  = '/content/drive/MyDrive/mushroom_masterv1_ftv1.4.keras'\n",
    "tflite_path ='/content/drive/MyDrive/mushroom_master_mobilev1.4.tflite'\n",
    "batch_size  = 128\n",
    "image_size  = (224, 224)\n",
    "\n",
    "def remove_hidden_files(folder_path):\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for d in dirs:\n",
    "            if d.startswith('.'):\n",
    "                shutil.rmtree(os.path.join(root, d))\n",
    "        for f in files:\n",
    "            if f.startswith('.'):\n",
    "                os.remove(os.path.join(root, f))\n",
    "\n",
    "def prune_empty_classes(folder_path, min_count=10):\n",
    "    for class_name in os.listdir(folder_path):\n",
    "        class_dir = os.path.join(folder_path, class_name)\n",
    "        if os.path.isdir(class_dir):\n",
    "            valid_images = [f for f in os.listdir(class_dir) if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "            if len(valid_images) < min_count:\n",
    "                shutil.rmtree(class_dir)\n",
    "\n",
    "def get_images_and_labels_local(local_dir):\n",
    "    class_names = sorted([\n",
    "        d for d in os.listdir(local_dir) if os.path.isdir(os.path.join(local_dir, d)) and not d.startswith('.')\n",
    "    ])\n",
    "    class_to_index = {name: idx for idx, name in enumerate(class_names)}\n",
    "    paths = []\n",
    "    labels = []\n",
    "    for class_name in class_names:\n",
    "        class_path = os.path.join(local_dir, class_name)\n",
    "        for root, _, files in os.walk(class_path):\n",
    "            for f in files:\n",
    "                if f.lower().endswith((\".jpg\", \".jpeg\", \".png\")) and not f.startswith('.'):\n",
    "                    rel_path = os.path.join(os.path.relpath(root, local_dir), f)\n",
    "                    paths.append(rel_path)\n",
    "                    labels.append(class_to_index[class_name])\n",
    "    return np.array(paths), np.array(labels), class_names, class_to_index\n",
    "\n",
    "remove_hidden_files(local_data_dir)\n",
    "prune_empty_classes(local_data_dir, min_count=1) # to remove any hidden files that might get into the model undetected\n",
    "\n",
    "all_image_paths, all_labels, class_names, class_to_index = get_images_and_labels_local(local_data_dir)\n",
    "\n",
    "train_val_paths, test_paths, train_val_labels, test_labels = train_test_split( # split into train and test\n",
    "    all_image_paths, all_labels, test_size=0.1, stratify=all_labels, shuffle=True, random_state=42)\n",
    "\n",
    "def load_and_preprocess_image(path, label):\n",
    "     full_path = tf.strings.join([\"/content/local_data/cleaned_dataset/\", path])\n",
    "     try:\n",
    "         image_raw = tf.io.read_file(full_path)\n",
    "         img = tf.image.decode_image(image_raw, channels=3, expand_animations=False)\n",
    "     except tf.errors.InvalidArgumentError:\n",
    "         # dummy image in case of error\n",
    "         img = tf.zeros((image_size[0], image_size[1], 3), dtype=tf.uint8)\n",
    "\n",
    "     img = tf.image.resize(img, image_size)\n",
    "     img = tf.cast(img, tf.float32) / 255.0\n",
    "     return img, label\n",
    "\n",
    "def create_dataset(image_paths, labels, shuffle=True, repeat=False):\n",
    "    # Converts lists of image paths and labels to tensors\n",
    "    image_paths = tf.convert_to_tensor(image_paths, dtype=tf.string)\n",
    "    labels = tf.convert_to_tensor(labels, dtype=tf.int32)\n",
    "\n",
    "    # Create a dataset from the image paths and labels\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(image_paths), seed=42)\n",
    "    if repeat:\n",
    "        dataset = dataset.repeat()\n",
    "\n",
    "    # Map the loading function which now returns only image and label\n",
    "    dataset = dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "test_ds = create_dataset(test_paths, test_labels, shuffle=False)\n",
    "\n",
    "print(f\"Loading model from {model_path} ...\")\n",
    "model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "\n",
    "test_loss, test_acc, test_top5 = model.evaluate(test_ds, verbose=2)\n",
    "print(f\"\\nTest loss      : {test_loss:.4f}\")\n",
    "print(f\"Test top‑1 acc : {test_acc:.4f}\")\n",
    "print(f\"Test top‑5 acc : {test_top5:.4f}\")\n",
    "\n",
    "y_true = np.concatenate([y.numpy() for _, y in test_ds])\n",
    "y_prob = model.predict(test_ds, verbose=0)\n",
    "y_pred = np.argmax(y_prob, axis=1)\n",
    "\n",
    "#create sorted list of model outputs to show best and worst performing classes\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "report_dict = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
    "\n",
    "df_report = pd.DataFrame(report_dict).transpose()\n",
    "\n",
    "df_per_class = df_report.iloc[:-3].copy()\n",
    "\n",
    "df_sorted = df_per_class.sort_values(by='f1-score')\n",
    "\n",
    "\n",
    "df_display = df_sorted[['precision', 'recall', 'f1-score', 'support']]\n",
    "df_display.style.format({\n",
    "    'precision': '{:.3f}',\n",
    "    'recall': '{:.3f}',\n",
    "    'f1-score': '{:.3f}',\n",
    "    'support': '{:.0f}'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8nev14R1Y_Sa",
    "outputId": "dddf1ffd-7277-4a48-e710-3a98c129fc5f"
   },
   "outputs": [],
   "source": [
    "!pip install ai-edge-litert\n",
    "from ai_edge_litert.interpreter import Interpreter\n",
    "\n",
    "#evaluate quantised model\n",
    "interpreter = Interpreter(model_path=tflite_path)\n",
    "interpreter.allocate_tensors()\n",
    "inp_idx  = interpreter.get_input_details()[0][\"index\"]\n",
    "out_idx  = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "top1_hits = top5_hits = n_samples = 0\n",
    "\n",
    "for x_batch, y_batch in test_ds:\n",
    "    x_batch = x_batch.numpy().astype(np.float32)\n",
    "    y_batch = y_batch.numpy()\n",
    "\n",
    "    for img, lbl in zip(x_batch, y_batch):\n",
    "        input_data = np.expand_dims(img, axis=0)\n",
    "        interpreter.set_tensor(inp_idx, input_data)\n",
    "        interpreter.invoke()\n",
    "        preds = interpreter.get_tensor(out_idx)[0]\n",
    "\n",
    "        top1 = np.argmax(preds)\n",
    "        top5 = np.argsort(preds)[-5:]\n",
    "\n",
    "        top1_hits += int(top1 == lbl)\n",
    "        top5_hits += int(lbl in top5)\n",
    "        n_samples += 1\n",
    "\n",
    "print(f\"\\nTFLite top‑1 acc : {top1_hits / n_samples:.4f}\")\n",
    "print(f\"TFLite top‑5 acc : {top5_hits / n_samples:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
