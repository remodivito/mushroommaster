{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1aSueGfEHFVv"
   },
   "source": [
    "## Data Preparation\n",
    "Classes are moderately imbalanced due to the nature of the dataset (aggregated on mushroomobserver.com), so effort has to be made to be intentional in splitting samples from each class fairly. A 0.8/0.1/0.1 split has been chosen to begin with due to the dataset being quite small https://encord.com/blog/train-val-test-split/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zFukUL_7_8_z",
    "outputId": "7d384d41-06bb-4eb6-cb5d-f579e36c1990"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import logging\n",
    "\n",
    "source_directory = '/content/drive/Othercomputers/My MacBook Pro/MO_MI_images'\n",
    "version = 'v1.1'\n",
    "\n",
    "def is_valid_file(filepath):\n",
    "    \"\"\"Check if file is a valid image.\"\"\"\n",
    "    if not os.path.basename(filepath).startswith('.') and os.path.isfile(filepath):\n",
    "        try:\n",
    "            img = Image.open(filepath)\n",
    "            img.verify()\n",
    "            img.close()\n",
    "            return True\n",
    "        except (IOError, SyntaxError):\n",
    "            logging.warning(f\"Invalid image file: {filepath}\")\n",
    "            return False\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def get_images_and_labels(source_dir, min_images_per_class=250):\n",
    "    class_names = [d for d in os.listdir(source_dir)\n",
    "                   if not d.startswith('.') and os.path.isdir(os.path.join(source_dir, d))]\n",
    "    class_names.sort()\n",
    "\n",
    "    # Filter classes based on minimum number of images\n",
    "    valid_class_names = []\n",
    "    class_to_image_paths = {}\n",
    "    for class_name in class_names:\n",
    "        class_dir = os.path.join(source_dir, class_name)\n",
    "        image_filenames = [f for f in os.listdir(class_dir)\n",
    "                           if is_valid_file(os.path.join(class_dir, f))]\n",
    "        if len(image_filenames) >= min_images_per_class: # this filters out most classes - will have to integrate more sources into dataset\n",
    "            valid_class_names.append(class_name)\n",
    "            image_paths = [os.path.join(class_dir, f) for f in image_filenames]\n",
    "            class_to_image_paths[class_name] = image_paths\n",
    "        else:\n",
    "            logging.warning(f\"Skipping class '{class_name}' (only {len(image_filenames)} images)\")\n",
    "\n",
    "    # Create a mapping from class names to label indices\n",
    "    class_names = sorted(valid_class_names)\n",
    "    class_to_index = {class_name: index for index, class_name in enumerate(class_names)}\n",
    "\n",
    "    # Collect all image paths and labels\n",
    "    all_image_paths = []\n",
    "    all_labels = []\n",
    "    for class_name in class_names:\n",
    "        image_paths = class_to_image_paths[class_name]\n",
    "        labels = [class_to_index[class_name]] * len(image_paths)\n",
    "        all_image_paths.extend(image_paths)\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "    return all_image_paths, all_labels, class_names, class_to_index\n",
    "\n",
    "all_image_paths, all_labels, class_names, class_to_index = get_images_and_labels(source_directory)\n",
    "num_classes = len(class_names)\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Total images: {len(all_image_paths)}\")\n",
    "\n",
    "# Save labels to file\n",
    "labels_file_path = f'/content/drive/MyDrive/labels{version}.txt'\n",
    "os.makedirs(os.path.dirname(labels_file_path), exist_ok=True)\n",
    "with open(labels_file_path, 'w') as f:\n",
    "    for label in class_names:\n",
    "        f.write(f\"{label}\\n\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert to numpy arrays\n",
    "all_image_paths = np.array(all_image_paths)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# First split into train/val and test\n",
    "train_val_paths, test_paths, train_val_labels, test_labels = train_test_split(\n",
    "    all_image_paths, all_labels, test_size=0.1, stratify=all_labels, shuffle=True, random_state=42) # Shuffling here may have been responsible for huge accuracy increase - so cv probably important\n",
    "\n",
    "# Then split train/val into train and val\n",
    "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "    train_val_paths, train_val_labels, test_size=0.1, stratify=train_val_labels, random_state=42)\n",
    "\n",
    "print(f\"Training samples: {len(train_paths)}\")\n",
    "print(f\"Validation samples: {len(val_paths)}\")\n",
    "print(f\"Test samples: {len(test_paths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAxnVVtsH0cE"
   },
   "source": [
    "## Data Preprocessing\n",
    "Data needs to be:\n",
    "\n",
    "\n",
    "*   Decoded from JPEGs into RGB grids of pixels\n",
    "*   From there converted into floating-point tensors\n",
    "*   Reshaped into a standard size\n",
    "*   Packed into batches\n",
    "\n",
    "This can be done by the keras function `image_dataset_from_directory`. This function also features the ability to shuffle the training data to expose the model to a more diverse set of examples. (https://www.markovml.com/glossary/data-shuffling)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DD_sapIgH0mo"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 1536 # massive batch size to make use of 300GB RAM V28 TPU\n",
    "image_size = (224, 224)\n",
    "\n",
    "def load_and_preprocess_image(path, label):\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, image_size)\n",
    "    image = image / 255.0  # Normalize to [0,1] range\n",
    "    return image, label\n",
    "\n",
    "def create_dataset(image_paths, labels, shuffle=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(image_paths), seed=42)\n",
    "    dataset = dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "train_ds = create_dataset(train_paths, train_labels, shuffle=True)\n",
    "val_ds = create_dataset(val_paths, val_labels, shuffle=False)\n",
    "test_ds = create_dataset(test_paths, test_labels, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8XN-uUuYsGAE"
   },
   "source": [
    "## Data Augmentation\n",
    "\n",
    "Data Augmentation is used to boost the size of the datasets to help account for the variability inherent in user-uploaded images. Keras_cv and inbuilt augmentation functions are used for random horizontal flips, rotations, random scalings, Gaussian blur,brighthenss shifts and random cutouts, to help expand the training dataset and replicate teh wide array of circumstances in which users will take or upload pictures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6-_L2pQ2dp_H",
    "outputId": "2ba39bff-9ccd-479a-e824-eaccd1a56df0"
   },
   "outputs": [],
   "source": [
    "!pip install keras-cv --upgrade\n",
    "import keras_cv\n",
    "import tensorflow as tf\n",
    "from keras_cv.layers import RandomApply\n",
    "from keras import layers\n",
    "\n",
    "rotation_factor = 30 / 360.0\n",
    "zoom_factor = 0.2  # Scaling between 80% and 120%\n",
    "\n",
    "data_augmentation_layers = [\n",
    "    layers.RandomFlip(\"horizontal\"),  # Random horizontal flip (probability of 0.5)\n",
    "    layers.RandomRotation(factor=(-rotation_factor, rotation_factor)),  # Random rotation\n",
    "    layers.RandomZoom(height_factor=(-zoom_factor, zoom_factor), width_factor=(-zoom_factor, zoom_factor)),  # Random scaling\n",
    "    RandomApply(\n",
    "        layer=keras_cv.layers.RandomGaussianBlur(kernel_size=3, factor=(0.0, 1.0)),\n",
    "        rate=0.5  # Apply with 50% probability\n",
    "    ),\n",
    "    keras_cv.layers.RandomBrightness(factor=(-0.3, 0.3)),  # Random brightness adjustment\n",
    "    RandomApply(\n",
    "        layer=keras_cv.layers.RandomCutout(\n",
    "            height_factor=(0.2, 0.2),\n",
    "            width_factor=(0.2, 0.2),\n",
    "        ),\n",
    "        rate=0.5\n",
    "    ),\n",
    "]\n",
    "\n",
    "def data_augmentation(images, targets):\n",
    "    for layer in data_augmentation_layers:\n",
    "        images = layer(images)\n",
    "    return images, targets\n",
    "\n",
    "# Apply data augmentation to training dataset\n",
    "augmented_train_ds = train_ds.map(data_augmentation, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "augmented_train_ds = augmented_train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hM-C5bbWeQiF"
   },
   "source": [
    "## Fine Tuning\n",
    "A fine-tuned version of the ConvNeXt-Base model - ConvNeXt is a CNN that implements characteristics of Visual Transformers to keep it comparable with state-of-the-art pure ViT architectures (https://arxiv.org/abs/2201.03545). The top 4 layers - the most abstract representations of the data - are then fine tuned and trained on the mushroom dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0SHkmd40dxEC",
    "outputId": "ddade47f-fb46-4a68-94e3-8f69ac46a718"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.applications import ConvNeXtBase\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Define paths for saving model and history\n",
    "model_path = f'/content/drive/MyDrive/mushroom_masterv1_ft{version}.keras'\n",
    "history_path = f'/content/drive/MyDrive/mushroom_training_history{version}.pkl'\n",
    "\n",
    "try: # initialise TPU if it is not already intialised\n",
    "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_cluster(resolver)\n",
    "    tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "    strategy = tf.distribute.TPUStrategy(resolver)\n",
    "    print(\"TPU already initialized. Reusing existing TPU strategy.\")\n",
    "except ValueError:\n",
    "    print(\"TPU not initialized. Skipping TPU initialization.\")\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
    "\n",
    "with strategy.scope(): # within TPU strategy defined above\n",
    "    # Define and compile the model\n",
    "    base_model = ConvNeXtBase(\n",
    "        input_shape=(224, 224, 3),\n",
    "        weights='imagenet',\n",
    "        include_top=False)\n",
    "\n",
    "    base_model.trainable = True\n",
    "    for layer in base_model.layers[:-4]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    inputs = keras.Input(shape=(224, 224, 3))\n",
    "    x = base_model(inputs)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(256)(x)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                  optimizer=keras.optimizers.Adam(learning_rate=0.1), # https://arxiv.org/pdf/1412.6980 - may need to adjust LR\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "# Early stopping callback - stops the model running unnecessarily\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=8,\n",
    "    start_from_epoch=10,\n",
    "    min_delta=0.05, # in first version, random fluctuations in loss would lead to this unnecesarily continuing\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Learning Rate Scheduling Callback - reduces learning rate when loss stops improving\n",
    "reduce_lr_plateau = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=4,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint( # good if training stops unexpectedly\n",
    "        filepath=model_path,\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\"),\n",
    "    early_stopping,\n",
    "    reduce_lr_plateau\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=50,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Save the training history\n",
    "with open(history_path, 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2L_xUJ1Boen"
   },
   "source": [
    "# Display Data\n",
    "\n",
    "Display the model validation accuracy and loss at each epoch of the training cycle, and accuracy against test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 962
    },
    "id": "AzfuD1axBoen",
    "outputId": "b15caca7-1ce6-401c-cb05-5fea33dc46c8"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "test_loss, test_acc = model.evaluate(test_ds, verbose=2)\n",
    "print(f\"Test accuracy: {test_acc}\")\n",
    "\n",
    "import pickle\n",
    "history_path = f'/content/drive/MyDrive/mushroom_training_history{version}.pkl'\n",
    "with open(history_path, 'rb') as f:\n",
    "    history = pickle.load(f)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history['accuracy'])\n",
    "plt.plot(history['val_accuracy'])\n",
    "plt.plot([test_acc] * len(history['accuracy']), linestyle='--')  # Horizontal line for test accuracy\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history['loss'])\n",
    "plt.plot(history['val_loss'])\n",
    "plt.plot([test_loss] * len(history['loss']), linestyle='--')  # Horizontal line for test loss\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCAyW5BoBoen"
   },
   "source": [
    "# Convert to TFLite\n",
    "The trained model must be converted to a TFLite file to be compatible with the mobile app. From https://medium.com/@hellokhorshed/a-step-by-step-guide-to-convert-keras-model-to-tensorflow-lite-tflite-model-6c8d08707488"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XWk9LyCiBoen"
   },
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "with open(f'/content/drive/MyDrive/mushroom_masterv1_mobile{version}.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
