{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ppREuCXXUUp"
   },
   "source": [
    "### V1.5\n",
    "This notebook removes oversampling to avoid overfitting on rare mushroom species, changes the base model from ConvNext-Small to EfficientNetV2S, adds some optimisations such as increased input image resolution, and improves model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1aSueGfEHFVv"
   },
   "source": [
    "## Data Preparation\n",
    "Classes are moderately imbalanced due to the nature of the dataset (aggregated on mushroomobserver.com), so effort has to be made to be intentional in splitting samples from each class fairly. A 0.8/0.1/0.1 split has been chosen to begin with due to the dataset being quite small https://encord.com/blog/train-val-test-split/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "zFukUL_7_8_z",
    "outputId": "b72e7ee8-90d0-4e58-b20c-166356fcc910"
   },
   "outputs": [],
   "source": [
    "!pip install -U crcmod\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from google.cloud import storage\n",
    "from io import BytesIO\n",
    "from google.colab import auth\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "auth.authenticate_user()\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!pip install --upgrade tensorflow\n",
    "\n",
    "project_id = \"mushroom-master-136c0\"\n",
    "bucket_name = \"mushroom-master-central\"\n",
    "source_directory = \"cleaned_dataset/\"\n",
    "\n",
    "\n",
    "client = storage.Client(project=project_id)\n",
    "bucket = client.bucket(bucket_name)\n",
    "\n",
    "!mkdir -p /content/local_data/cleaned_dataset\n",
    "!gsutil -m rsync -r gs://{bucket_name}/{source_directory} /content/local_data/cleaned_dataset\n",
    "\n",
    "local_data_dir = \"/content/local_data/cleaned_dataset\"\n",
    "\n",
    "def remove_hidden_files(folder_path):\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for d in dirs:\n",
    "            if d.startswith('.'):\n",
    "                shutil.rmtree(os.path.join(root, d))\n",
    "        for f in files:\n",
    "            if f.startswith('.'):\n",
    "                os.remove(os.path.join(root, f))\n",
    "\n",
    "def prune_empty_classes(folder_path, min_count=10):\n",
    "    for class_name in os.listdir(folder_path):\n",
    "        class_dir = os.path.join(folder_path, class_name)\n",
    "        if os.path.isdir(class_dir):\n",
    "            valid_images = [f for f in os.listdir(class_dir) if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "            if len(valid_images) < min_count:\n",
    "                shutil.rmtree(class_dir)\n",
    "\n",
    "def get_images_and_labels_local(local_dir):\n",
    "    class_names = sorted([\n",
    "        d for d in os.listdir(local_dir) if os.path.isdir(os.path.join(local_dir, d)) and not d.startswith('.')\n",
    "    ])\n",
    "    class_to_index = {name: idx for idx, name in enumerate(class_names)}\n",
    "    paths = []\n",
    "    labels = []\n",
    "    for class_name in class_names:\n",
    "        class_path = os.path.join(local_dir, class_name)\n",
    "        for root, _, files in os.walk(class_path):\n",
    "            for f in files:\n",
    "                if f.lower().endswith((\".jpg\", \".jpeg\", \".png\")) and not f.startswith('.'):\n",
    "                    rel_path = os.path.join(os.path.relpath(root, local_dir), f)\n",
    "                    paths.append(rel_path)\n",
    "                    labels.append(class_to_index[class_name])\n",
    "    return np.array(paths), np.array(labels), class_names, class_to_index\n",
    "\n",
    "remove_hidden_files(local_data_dir)\n",
    "prune_empty_classes(local_data_dir, min_count=1) # to remove any hidden files that might get into the model undetected\n",
    "\n",
    "all_image_paths, all_labels, class_names, class_to_index = get_images_and_labels_local(local_data_dir)\n",
    "\n",
    "train_val_paths, test_paths, train_val_labels, test_labels = train_test_split( # split into train and test\n",
    "    all_image_paths, all_labels, test_size=0.1, stratify=all_labels, shuffle=True, random_state=42)\n",
    "\n",
    "train_paths, val_paths, train_labels, val_labels = train_test_split( # split train into train and val - this actually results in a 0.81/0.09/0.1 split but this is rounded to 0.8/0.1/0.1 for simplicity and readability\n",
    "    train_val_paths, train_val_labels, test_size=0.1, stratify=train_val_labels, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAxnVVtsH0cE"
   },
   "source": [
    "## Data Preprocessing\n",
    "Data needs to be:\n",
    "\n",
    "\n",
    "*   Decoded from JPEGs into RGB grids of pixels\n",
    "*   From there converted into floating-point tensors\n",
    "*   Reshaped into a standard size\n",
    "*   Packed into batches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "DD_sapIgH0mo",
    "outputId": "caf6ec09-6bfb-4515-cf3c-4d15159671bb"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "version = \"v1.5\"\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "batch_size = 128\n",
    "image_size = (384, 384)\n",
    "\n",
    "def load_and_preprocess_image(path, label):\n",
    "     full_path = tf.strings.join([\"/content/local_data/cleaned_dataset/\", path])\n",
    "     try:\n",
    "         image_raw = tf.io.read_file(full_path)\n",
    "         img = tf.image.decode_image(image_raw, channels=3, expand_animations=False)\n",
    "     except tf.errors.InvalidArgumentError:\n",
    "         # dummy image in case of error\n",
    "         img = tf.zeros((image_size[0], image_size[1], 3), dtype=tf.uint8)\n",
    "\n",
    "     img = tf.image.resize(img, image_size)\n",
    "     img = tf.cast(img, tf.float32) / 255.0\n",
    "     return img, label\n",
    "\n",
    "def create_dataset(image_paths, labels, shuffle=True, repeat=False):\n",
    "    # Converts lists of image paths and labels to tensors\n",
    "    image_paths = tf.convert_to_tensor(image_paths, dtype=tf.string)\n",
    "    labels = tf.convert_to_tensor(labels, dtype=tf.int32)\n",
    "\n",
    "    # Create a dataset from the image paths and labels\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(image_paths), seed=42)\n",
    "    if repeat:\n",
    "        dataset = dataset.repeat()\n",
    "\n",
    "    # Map the loading function which now returns only image and label\n",
    "    dataset = dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "# Create the datasets\n",
    "train_ds = create_dataset(train_paths, train_labels, shuffle=True, repeat=True)\n",
    "val_ds = create_dataset(val_paths, val_labels, shuffle=False)\n",
    "test_ds = create_dataset(test_paths, test_labels, shuffle=False)\n",
    "\n",
    "print(\"Datasets created!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8XN-uUuYsGAE"
   },
   "source": [
    "## Data Augmentation\n",
    "\n",
    "Data Augmentation is used to boost the size of the datasets to help account for the variability inherent in user-uploaded images. The keras.layers library is used for its flexibility - moderately strong augmentations are performed, striking a balance between expanding the dataset and computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "6-_L2pQ2dp_H"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "\n",
    "rotation_factor = 30 / 360.0\n",
    "zoom_factor = 0.2  # Scaling between 80% and 120%\n",
    "\n",
    "data_augmentation_layers = [\n",
    "    layers.RandomFlip(\"horizontal\"),  # Random horizontal flip (probability of 0.5)\n",
    "    layers.RandomRotation(factor=(-rotation_factor, rotation_factor)),  # Random rotation\n",
    "    layers.RandomZoom(height_factor=(-zoom_factor, zoom_factor), width_factor=(-zoom_factor, zoom_factor)),  # Random scaling\n",
    "    # keras_cv.layers.RandomBrightness(factor=(-0.3, 0.3)),  # Random brightness adjustment\n",
    "]\n",
    "\n",
    "def data_augmentation(images, targets):\n",
    "    for layer in data_augmentation_layers:\n",
    "        images = layer(images)\n",
    "    # Clip values to keep between 0 and 1\n",
    "    images = tf.clip_by_value(images, 0.0, 1.0)\n",
    "    return images, targets\n",
    "\n",
    "\n",
    "# Apply data augmentation to training dataset\n",
    "augmented_train_ds = train_ds.map(data_augmentation, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "augmented_train_ds = augmented_train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ymy9rquFoJw8"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "classes = np.unique(train_labels)\n",
    "weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=classes,\n",
    "    y=train_labels\n",
    ")\n",
    "\n",
    "class_weight = { int(c): w for c, w in zip(classes, weights) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hM-C5bbWeQiF"
   },
   "source": [
    "## Fine Tuning\n",
    "A fine-tuned version of the ConvNeXt-Small model - ConvNeXt is a CNN that implements characteristics of Visual Transformers to keep it comparable with state-of-the-art pure ViT architectures (https://arxiv.org/abs/2201.03545). The top 4 layers - the most abstract representations of the data - are then fine tuned and trained on the mushroom dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "gPCdh4xm_rNo",
    "outputId": "ea9d9f39-2bc1-47e3-f1c7-39d36a94d672"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.applications import EfficientNetV2S\n",
    "import pickle\n",
    "\n",
    "# Paths\n",
    "model_path      = '/content/drive/MyDrive/mushroom_masterv1_ftv1.5.keras'\n",
    "history_path    = '/content/drive/MyDrive/mushroom_training_historyv1.5.pkl'\n",
    "\n",
    "# load last model checkpoint\n",
    "model = keras.models.load_model(model_path) # training was interrupted by runnin gout of compute - had to reload checkpoint\n",
    "\n",
    "model.compile(\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=1e-4, clipnorm=1.0),\n",
    "    metrics = [\"accuracy\", keras.metrics.SparseTopKCategoricalAccuracy(5, name='top_5_accuracy')]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=11,\n",
    "    start_from_epoch=10,\n",
    "    min_delta=0.005,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr_plateau = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=4,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "terminate_nan = tf.keras.callbacks.TerminateOnNaN()\n",
    "\n",
    "steps_per_epoch = len(train_paths) // batch_size\n",
    "validation_steps = len(val_paths) // batch_size if len(val_paths) > batch_size else None\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=model_path,\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\"\n",
    "    ),\n",
    "    early_stopping,\n",
    "    reduce_lr_plateau,\n",
    "    terminate_nan\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    augmented_train_ds,\n",
    "    epochs=140,\n",
    "    initial_epoch=33,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_ds,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weight\n",
    ")\n",
    "\n",
    "\n",
    "# Save training history\n",
    "with open(history_path, 'wb') as f:\n",
    "    pickle.dump(history.history, f)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
