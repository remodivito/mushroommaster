{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ppREuCXXUUp"
   },
   "source": [
    "### V1.4\n",
    "This model utilises oversampling to address class imbalance and utilises data augmentation to improve the model's generalisation. The dataset now also includes the Danish Fungi dataset, and has been denoised to get rid of irrelevant images. Additionally, this notebook is to be trained on a more powerful A100 GPU, as opposed to a V28 TPU, for more efficient convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1aSueGfEHFVv"
   },
   "source": [
    "## Data Preparation\n",
    "Classes are moderately imbalanced due to the nature of the dataset (aggregated on mushroomobserver.com), so effort has to be made to be intentional in splitting samples from each class fairly. A 0.8/0.1/0.1 split has been chosen to begin with due to the dataset being quite small https://encord.com/blog/train-val-test-split/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zFukUL_7_8_z",
    "outputId": "1f144c83-9747-4786-8be6-f68342f4341b"
   },
   "outputs": [],
   "source": [
    "!pip install -U crcmod\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from google.cloud import storage\n",
    "from io import BytesIO\n",
    "from google.colab import auth\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "auth.authenticate_user()\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!pip install --upgrade tensorflow\n",
    "\n",
    "project_id = \"mushroom-master-136c0\"\n",
    "bucket_name = \"mushroom-master-central\"\n",
    "source_directory = \"cleaned_dataset/\"\n",
    "version = \"v1.4\"\n",
    "\n",
    "client = storage.Client(project=project_id)\n",
    "bucket = client.bucket(bucket_name)\n",
    "\n",
    "!mkdir -p /content/local_data/cleaned_dataset\n",
    "!gsutil -m rsync -r gs://{bucket_name}/{source_directory} /content/local_data/cleaned_dataset\n",
    "\n",
    "local_data_dir = \"/content/local_data/cleaned_dataset\"\n",
    "\n",
    "def remove_hidden_files(folder_path):\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for d in dirs:\n",
    "            if d.startswith('.'):\n",
    "                shutil.rmtree(os.path.join(root, d))\n",
    "        for f in files:\n",
    "            if f.startswith('.'):\n",
    "                os.remove(os.path.join(root, f))\n",
    "\n",
    "def prune_empty_classes(folder_path, min_count=10):\n",
    "    for class_name in os.listdir(folder_path):\n",
    "        class_dir = os.path.join(folder_path, class_name)\n",
    "        if os.path.isdir(class_dir):\n",
    "            valid_images = [f for f in os.listdir(class_dir) if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "            if len(valid_images) < min_count:\n",
    "                shutil.rmtree(class_dir)\n",
    "\n",
    "def get_images_and_labels_local(local_dir):\n",
    "    class_names = sorted([\n",
    "        d for d in os.listdir(local_dir) if os.path.isdir(os.path.join(local_dir, d)) and not d.startswith('.')\n",
    "    ])\n",
    "    class_to_index = {name: idx for idx, name in enumerate(class_names)}\n",
    "    paths = []\n",
    "    labels = []\n",
    "    for class_name in class_names:\n",
    "        class_path = os.path.join(local_dir, class_name)\n",
    "        for root, _, files in os.walk(class_path):\n",
    "            for f in files:\n",
    "                if f.lower().endswith((\".jpg\", \".jpeg\", \".png\")) and not f.startswith('.'):\n",
    "                    rel_path = os.path.join(os.path.relpath(root, local_dir), f)\n",
    "                    paths.append(rel_path)\n",
    "                    labels.append(class_to_index[class_name])\n",
    "    return np.array(paths), np.array(labels), class_names, class_to_index\n",
    "\n",
    "remove_hidden_files(local_data_dir)\n",
    "prune_empty_classes(local_data_dir, min_count=1) # to remove any hidden files that might get into the model undetected\n",
    "\n",
    "all_image_paths, all_labels, class_names, class_to_index = get_images_and_labels_local(local_data_dir)\n",
    "\n",
    "train_val_paths, test_paths, train_val_labels, test_labels = train_test_split( # split into train and test\n",
    "    all_image_paths, all_labels, test_size=0.1, stratify=all_labels, shuffle=True, random_state=42)\n",
    "\n",
    "train_paths, val_paths, train_labels, val_labels = train_test_split( # split train into train and val - this actually results in a 0.81/0.09/0.1 split but this is rounded to 0.8/0.1/0.1 for simplicity and readability\n",
    "    train_val_paths, train_val_labels, test_size=0.1, stratify=train_val_labels, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gFJNcmPCPUR"
   },
   "source": [
    "## Oversampling\n",
    "This section utilises oversampling to help negate the negative effect of the highly imbalanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NO6_sfLgCAwo"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def oversample_data(paths, labels):\n",
    "    class_counts = Counter(labels)\n",
    "    max_count = max(class_counts.values())\n",
    "\n",
    "    new_paths = []\n",
    "    new_labels = []\n",
    "    for c, count in class_counts.items():\n",
    "        c_indices = np.where(labels == c)[0]\n",
    "        c_paths = paths[c_indices]\n",
    "        c_labels = labels[c_indices]\n",
    "\n",
    "        replicate_factor = int(np.ceil(max_count / count))\n",
    "\n",
    "        c_paths_oversampled = np.tile(c_paths, replicate_factor)\n",
    "        c_labels_oversampled = np.tile(c_labels, replicate_factor)\n",
    "\n",
    "        #trims if oversampling overshot\n",
    "        c_paths_oversampled = c_paths_oversampled[:max_count]\n",
    "        c_labels_oversampled = c_labels_oversampled[:max_count]\n",
    "\n",
    "        new_paths.extend(c_paths_oversampled)\n",
    "        new_labels.extend(c_labels_oversampled)\n",
    "\n",
    "    # shuffle and convert to numpy array\n",
    "    new_paths = np.array(new_paths)\n",
    "    new_labels = np.array(new_labels)\n",
    "    p = np.random.permutation(len(new_paths))\n",
    "    new_paths = new_paths[p]\n",
    "    new_labels = new_labels[p]\n",
    "    return new_paths, new_labels\n",
    "\n",
    "# Oversample the *training* set only\n",
    "train_paths, train_labels = oversample_data(train_paths, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAxnVVtsH0cE"
   },
   "source": [
    "## Data Preprocessing\n",
    "Data needs to be:\n",
    "\n",
    "\n",
    "*   Decoded from JPEGs into RGB grids of pixels\n",
    "*   From there converted into floating-point tensors\n",
    "*   Reshaped into a standard size\n",
    "*   Packed into batches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DD_sapIgH0mo",
    "outputId": "00543b71-cab1-4416-8ea0-48985182b833"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "batch_size = 128\n",
    "image_size = (224, 224)\n",
    "\n",
    "def load_and_preprocess_image(path, label):\n",
    "     full_path = tf.strings.join([\"/content/local_data/cleaned_dataset/\", path])\n",
    "     try:\n",
    "         image_raw = tf.io.read_file(full_path)\n",
    "         img = tf.image.decode_image(image_raw, channels=3, expand_animations=False)\n",
    "     except tf.errors.InvalidArgumentError:\n",
    "         # dummy image in case of error\n",
    "         img = tf.zeros((image_size[0], image_size[1], 3), dtype=tf.uint8)\n",
    "\n",
    "     img = tf.image.resize(img, image_size)\n",
    "     img = tf.cast(img, tf.float32) / 255.0\n",
    "     return img, label\n",
    "\n",
    "def create_dataset(image_paths, labels, shuffle=True, repeat=False):\n",
    "    # Converts lists of image paths and labels to tensors\n",
    "    image_paths = tf.convert_to_tensor(image_paths, dtype=tf.string)\n",
    "    labels = tf.convert_to_tensor(labels, dtype=tf.int32)\n",
    "\n",
    "    # Create a dataset from the image paths and labels\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(image_paths), seed=42)\n",
    "    if repeat:\n",
    "        dataset = dataset.repeat()\n",
    "\n",
    "    # Map the loading function which now returns only image and label\n",
    "    dataset = dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "# Create the datasets\n",
    "train_ds = create_dataset(train_paths, train_labels, shuffle=True, repeat=True)\n",
    "val_ds = create_dataset(val_paths, val_labels, shuffle=False)\n",
    "test_ds = create_dataset(test_paths, test_labels, shuffle=False)\n",
    "\n",
    "print(\"Datasets created!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8XN-uUuYsGAE"
   },
   "source": [
    "## Data Augmentation\n",
    "\n",
    "Data Augmentation is used to boost the size of the datasets to help account for the variability inherent in user-uploaded images. The keras.layers library is used for its flexibility - moderately strong augmentations are performed, striking a balance between expanding the dataset and computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6-_L2pQ2dp_H"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "\n",
    "rotation_factor = 30 / 360.0\n",
    "zoom_factor = 0.2  # Scaling between 80% and 120%\n",
    "\n",
    "data_augmentation_layers = [\n",
    "    layers.RandomFlip(\"horizontal\"),  # Random horizontal flip (probability of 0.5)\n",
    "    layers.RandomRotation(factor=(-rotation_factor, rotation_factor)),  # Random rotation\n",
    "    layers.RandomZoom(height_factor=(-zoom_factor, zoom_factor), width_factor=(-zoom_factor, zoom_factor)),  # Random scaling\n",
    "    # keras_cv.layers.RandomBrightness(factor=(-0.3, 0.3)),  # Random brightness adjustment\n",
    "]\n",
    "\n",
    "def data_augmentation(images, targets):\n",
    "    for layer in data_augmentation_layers:\n",
    "        images = layer(images)\n",
    "    # Clip values to keep between 0 and 1\n",
    "    images = tf.clip_by_value(images, 0.0, 1.0)\n",
    "    return images, targets\n",
    "\n",
    "\n",
    "# Apply data augmentation to training dataset\n",
    "augmented_train_ds = train_ds.map(data_augmentation, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "augmented_train_ds = augmented_train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hM-C5bbWeQiF"
   },
   "source": [
    "## Fine Tuning\n",
    "A fine-tuned version of the ConvNeXt-Small model - ConvNeXt is a CNN that implements characteristics of Visual Transformers to keep it comparable with state-of-the-art pure ViT architectures (https://arxiv.org/abs/2201.03545). The top 4 layers - the most abstract representations of the data - are then fine tuned and trained on the mushroom dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "d9-t-f_ae3Sk",
    "outputId": "fe7cf038-65c0-4dbc-ba3c-5df3bde6f452"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.applications import ConvNeXtSmall\n",
    "import pickle\n",
    "\n",
    "model_path = f'/content/drive/MyDrive/mushroom_masterv1_ft{version}.keras'\n",
    "history_path = f'/content/drive/MyDrive/mushroom_training_history{version}.pkl'\n",
    "\n",
    "\n",
    "#with strategy.scope():\n",
    "base_model = ConvNeXtSmall(\n",
    "    input_shape=(224, 224, 3),\n",
    "    weights='imagenet',\n",
    "    include_top=False\n",
    ")\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "\n",
    "inputs = keras.Input(shape=(224, 224, 3))\n",
    "x = base_model(inputs)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "outputs = layers.Dense(len(class_names), activation='softmax')(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001, clipnorm=1.0),\n",
    "    metrics=[\"accuracy\", tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5, name='top_5_accuracy')]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=8,\n",
    "    start_from_epoch=10,\n",
    "    min_delta=0.005,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr_plateau = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=4,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "terminate_nan = tf.keras.callbacks.TerminateOnNaN()\n",
    "\n",
    "steps_per_epoch = len(train_paths) // batch_size\n",
    "validation_steps = len(val_paths) // batch_size if len(val_paths) > batch_size else None\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=model_path,\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\"\n",
    "    ),\n",
    "    early_stopping,\n",
    "    reduce_lr_plateau,\n",
    "    terminate_nan\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    augmented_train_ds,\n",
    "    epochs=150,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_ds,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Save training history\n",
    "with open(history_path, 'wb') as f:\n",
    "    pickle.dump(history.history, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2L_xUJ1Boen"
   },
   "source": [
    "# Display Data\n",
    "\n",
    "Display the model validation accuracy and loss at each epoch of the training cycle, and accuracy against test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AzfuD1axBoen"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "test_loss, test_acc, test_top_5_acc = model.evaluate(test_ds, verbose=2)\n",
    "print(f\"Test accuracy: {test_acc}, Test Top 5 Accuracy: {test_top_5_acc}\")\n",
    "\n",
    "import pickle\n",
    "history_path = f'/content/drive/MyDrive/mushroom_training_history{version}.pkl'\n",
    "with open(history_path, 'rb') as f:\n",
    "    history = pickle.load(f)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history['accuracy'])\n",
    "plt.plot(history['val_accuracy'])\n",
    "plt.plot(history['top_5_accuracy'])\n",
    "plt.plot(history['val_top_5_accuracy'])\n",
    "plt.plot([test_acc] * len(history['accuracy']), linestyle='--')  # Horizontal line for test accuracy\n",
    "plt.plot([test_top_5_acc] * len(history['accuracy']), linestyle='--')  # Horizontal line for test accuracy\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation', 'Top 5', 'Val Top 5', 'Test', 'Test Top 5'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history['loss'])\n",
    "plt.plot(history['val_loss'])\n",
    "plt.plot([test_loss] * len(history['loss']), linestyle='--')  # Horizontal line for test loss\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCAyW5BoBoen"
   },
   "source": [
    "# Convert to TFLite\n",
    "The trained model must be converted to a TFLite file to be compatible with the mobile app. From https://medium.com/@hellokhorshed/a-step-by-step-guide-to-convert-keras-model-to-tensorflow-lite-tflite-model-6c8d08707488"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XWk9LyCiBoen"
   },
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "with open(f'/content/drive/MyDrive/mushroom_master_mobile{version}.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YaJX4ZhYlPu"
   },
   "source": [
    "## Advanced Evaluation\n",
    "As this is the model that is being used with the final build of the app, a more advanced evaluation is taken, including building a confusion matrix and evaluating the .tflite model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w7iRim2IYU56"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "version    = \"v1.4\"\n",
    "model_path = f'/content/drive/MyDrive/mushroom_masterv1_ft{version}.keras'\n",
    "\n",
    "# since this is evaluation is taking place after the initial traiing, the model needs to be re-loaded in to colab and re-compiled\n",
    "model = tf.keras.models.load_model(model_path, compile=False)\n",
    "\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001, clipnorm=1.0),\n",
    "    metrics=[\"accuracy\", tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5, name='top_5_accuracy')]\n",
    ")\n",
    "\n",
    "\n",
    "# confusion matrix and classificaiton report using scikit-learn and seaborn (for CM heatmap)\n",
    "y_true = np.concatenate([y.numpy() for _, y in test_ds])\n",
    "y_prob = model.predict(test_ds, verbose=0)\n",
    "y_pred = np.argmax(y_prob, axis=1)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=range(len(class_names)))\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=class_names,\n",
    "    yticklabels=class_names,\n",
    "    square=True,\n",
    "    cbar=False,\n",
    ")\n",
    "plt.title(\"Confusion matrix — test set\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=class_names, digits=3))\n",
    "\n",
    "\n",
    "# evaluate TFLite model as the post-traing quantisation process may effect accuracy\n",
    "tflite_path = f'/content/drive/MyDrive/mushroom_master_mobile{version}.tflite'\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "interpreter.allocate_tensors()\n",
    "input_idx  = interpreter.get_input_details()[0][\"index\"]\n",
    "output_idx = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "top1_hits = top5_hits = n_samples = 0\n",
    "for batch_x, batch_y in test_ds:\n",
    "    interpreter.set_tensor(input_idx, batch_x.numpy().astype(np.float32))\n",
    "    interpreter.invoke()\n",
    "    preds = interpreter.get_tensor(output_idx)\n",
    "\n",
    "    top1 = np.argmax(preds, axis=1)\n",
    "    top5 = np.argsort(preds, axis=1)[:, -5:]\n",
    "\n",
    "    y_np = batch_y.numpy()\n",
    "    top1_hits += np.sum(top1 == y_np)\n",
    "    top5_hits += np.sum([label in p5 for label, p5 in zip(y_np, top5)])\n",
    "    n_samples += y_np.size\n",
    "\n",
    "print(f\"TFLite top‑1 accuracy : {top1_hits / n_samples:.4f}\")\n",
    "print(f\"TFLite top‑5 accuracy : {top5_hits / n_samples:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
